{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db8736a7-ed94-441c-9556-831fa57b5a10",
   "metadata": {},
   "source": [
    "Fine-Tuning Frontier Models\n",
    "===========================\n",
    "\n",
    "3 states to fine-tuning with OpenAI:\n",
    "\n",
    "1. Create training dataset in jsonl format and upload to OpenAI\n",
    "2. Run training - training loss and validation loss should decrease\n",
    "3. Evaluate results, tweak and repeat\n",
    "\n",
    "Key objectives of fine-tuning for Fronier models:\n",
    "\n",
    "- Setting style or tone in a way that can't be acheived with prompting\n",
    "- Improving the reliability of producing a type of output\n",
    "- Correcting failures to follow complex prompts\n",
    "- Handling edge cases\n",
    "- Performing a new skill or task that's hard to articulate in a prompt\n",
    "\n",
    "## Result\n",
    "\n",
    "<img src=\"./../images/Product-Pricer-Fine-Tuned-Results-GPT-Mini-Test-1.jpg\" alt=\"Distribution of Prices Predicted Using Fine Tuned GPT 4o Mini\" />\n",
    "\n",
    "Result: Does worse than untrained Frontier models but better than most traditional ML methods and improved in reducing large outliers.\n",
    "\n",
    "- GPT Fine Tuned Pricer Error=$101.59\n",
    "- RMSLE=0.80\n",
    "- Hits=41.6%\n",
    "\n",
    "## Compare Results to Basline Models\n",
    "\n",
    "<img src=\"./../images/Product-Pricer-FIne-Tuned-Baseline-Traditional-ML-Models-Results.jpg\" alt=\"Average Absolute Prediction Error Baseline Models vs FIne Tuned Frontier Model\" />\n",
    "\n",
    "A problem like ours doesn't benefit significantly from Fine Tuning\n",
    "\n",
    "- The problem and style of output can be clearly specified in a prompt\n",
    "- The model can take advantage of its enourmous world knowldge from its pre-training; providing a few hundred prices doesn't help\n",
    "\n",
    "# Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681c717b-4c24-4ac3-a5f3-3c5881d6e70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "import json\n",
    "import random\n",
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import login\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "from collections import Counter\n",
    "from openai import OpenAI\n",
    "from anthropic import Anthropic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70479b80-4210-440c-bef7-c0e5950b9796",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d05bdc-0155-4c72-a7ee-aa4e614ffd3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# environment\n",
    "\n",
    "load_dotenv(override=True)\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY', 'your-key-if-not-using-env')\n",
    "os.environ['ANTHROPIC_API_KEY'] = os.getenv('ANTHROPIC_API_KEY', 'your-key-if-not-using-env')\n",
    "os.environ['HF_TOKEN'] = os.getenv('HF_TOKEN', 'your-key-if-not-using-env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd3aad2-6f99-433c-8792-e461d2f06622",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log in to HuggingFace\n",
    "\n",
    "hf_token = os.environ['HF_TOKEN']\n",
    "login(hf_token, add_to_git_credential=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884a50bd-8cae-425e-8e56-f079fc3e65ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# moved our Tester into a separate package\n",
    "# call it with Tester.test(function_name, test_dataset)\n",
    "import sys\n",
    "sys.path.append('../testing/')\n",
    "from items import Item\n",
    "from testing import Tester"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a6fb86-74a4-403c-ab25-6db2d74e9d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c830ed3e-24ee-4af6-a07b-a1bfdcd39278",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea347df4-8809-434c-8bf5-027fd81c9ee0",
   "metadata": {},
   "source": [
    "# Load Curated Datasets\n",
    "\n",
    "Load in the pickle files create during data curation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9b05f4-c9eb-462c-8d86-de9140a2d985",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the pickle files:\n",
    "\n",
    "with open('../data/large-datasets/train.pkl', 'rb') as file:\n",
    "    train = pickle.load(file)\n",
    "\n",
    "with open('../data/large-datasets/test.pkl', 'rb') as file:\n",
    "    test = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8367135-f40e-43e1-8f3c-09e990ab1194",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI recommends fine-tuning with populations of 50-100 examples\n",
    "# But as our examples are very small, I'm suggesting we go with 200 examples (and 1 epoch)\n",
    "\n",
    "fine_tune_train = train[:200]\n",
    "fine_tune_validation = train[200:250]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be4a889-81c3-42b1-a2fc-034cdc7321a6",
   "metadata": {},
   "source": [
    "# Fine-Tuning GTP\n",
    "\n",
    "## Step 1: Preparing the Data\n",
    "\n",
    "Prepare our data for fine-tuning in JSONL (JSON Lines) format and upload to OpenAI.\n",
    "\n",
    "OpenAI expects data in JSONL format: rows of JSON each containing messages in the usual prompt format.\n",
    "\n",
    "    {\"messages\" : [{\"role\": \"system\", \"content\": \"You estimate prices...\"}]}\n",
    "    {\"messages\" : [{\"role\": \"system\", \"content\": \"You estimate prices...\"}]}\n",
    "    {\"messages\" : [{\"role\": \"system\", \"content\": \"You estimate prices...\"}]}\n",
    "\n",
    "### Prepare our data for fine-tuning in JSONL (JSON Lines) format and upload to OpenAI\n",
    "\n",
    "1. Create a good prompt for a Frontier model - when we train our own models, we'll need to make the problem as easy as possible, but a Frontier model needs no such simplification.\n",
    "\n",
    "> [{'role': 'system',\n",
    ">\n",
    "> 'content': 'You estimate prices of items. Reply only with the price, no explanation'},\n",
    "> {'role': 'user',\n",
    "> 'content': 'How much does this cost?\\n\\nDelphi FG0166 Fuel Pump Module\\nDelphi brings 80 years of OE Heritage into each Delphi pump, ensuring quality and fitment for each Delphi part. Part is validated, tested and matched to the right vehicle application Delphi brings 80 years of OE Heritage into each Delphi assembly, ensuring quality and fitment for each Delphi part Always be sure to check and clean fuel tank to avoid unnecessary returns Rigorous OE-testing ensures the pump can withstand extreme temperatures Brand Delphi, Fit Type Vehicle Specific Fit, Dimensions LxWxH 19.7 x 7.7 x 5.1 inches, Weight 2.2 Pounds, Auto Part Position Unknown, Operation Mode Mechanical, Manufacturer Delphi, Model FUEL PUMP, Dimensions 19.7'},\n",
    "> {'role': 'assistant', 'content': 'Price is $226.95'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae2fb3c-1cff-4ce3-911e-627c970edd7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First let's work on a good prompt for a Frontier model\n",
    "# Notice that I'm removing the \" to the nearest dollar\"\n",
    "# When we train our own models, we'll need to make the problem as easy as possible, \n",
    "# but a Frontier model needs no such simplification.\n",
    "\n",
    "def messages_for(item):\n",
    "    system_message = \"You estimate prices of items. Reply only with the price, no explanation\"\n",
    "    user_prompt = item.test_prompt().replace(\" to the nearest dollar\",\"\").replace(\"\\n\\nPrice is $\",\"\")\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "        {\"role\": \"assistant\", \"content\": f\"Price is ${item.price:.2f}\"}\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa280f6-1227-426a-a2e2-1ce985feba1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages_for(train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d831f4-7a38-4db5-880d-f1f9b4e9d3cb",
   "metadata": {},
   "source": [
    "2. Convert the items into a list of json objects - a \"jsonl\" string. Each row represents a message in the form: `{\"messages\" : [{\"role\": \"system\", \"content\": \"You estimate prices...`\n",
    "\n",
    "> {\"messages\": [{\"role\": \"system\", \"content\": \"You estimate prices of items. Reply only with the price, no explanation\"}, {\"role\": \"user\", \"content\": \"How much does this cost?\\n\\nDelphi FG0166 Fuel Pump Module\\nDelphi brings 80 years of OE Heritage into each Delphi pump, ensuring quality and fitment for each Delphi part. Part is validated, tested and matched to the right vehicle application Delphi brings 80 years of OE Heritage into each Delphi assembly, ensuring quality and fitment for each Delphi part Always be sure to check and clean fuel tank to avoid unnecessary returns Rigorous OE-testing ensures the pump can withstand extreme temperatures Brand Delphi, Fit Type Vehicle Specific Fit, Dimensions LxWxH 19.7 x 7.7 x 5.1 inches, Weight 2.2 Pounds, Auto Part Position Unknown, Operation Mode Mechanical, Manufacturer Delphi, Model FUEL PUMP, Dimensions 19.7\"}, {\"role\": \"assistant\", \"content\": \"Price is $226.95\"}]}<br>\n",
    "> {\"messages\": [{\"role\": \"system\", \"content\": \"You estimate prices of items. Reply only with the price, no explanation\"}, {\"role\": \"user\", \"content\": \"How much does this cost?\\n\\nPower Stop Rear Z36 Truck and Tow Brake Kit with Calipers\\nThe Power Stop Z36 Truck & Tow Performance brake kit provides the superior stopping power demanded by those who tow boats, haul loads, tackle mountains, lift trucks, and play in the harshest conditions. The brake rotors are drilled to keep temperatures down during extreme braking and slotted to sweep away any debris for constant pad contact. Combined with our Z36 Carbon-Fiber Ceramic performance friction formulation, you can confidently push your rig to the limit and look good doing it with red powder brake calipers. Components are engineered to handle the stress of towing, hauling, mountainous driving, and lifted trucks. Dust-free braking performance. Z36 Carbon-Fiber Ceramic formula provides the extreme braking performance demanded by your truck or 4x\"}, {\"role\": \"assistant\", \"content\": \"Price is $506.98\"}]}<br>\n",
    "> {\"messages\": [{\"role\": \"system\", \"content\": \"You estimate prices of items. Reply only with the price, no explanation\"}, {\"role\": \"user\", \"content\": \"How much does this cost?\\n\\nABBA 36 Gas Cooktop with 5 Sealed Burners - Tempered Glass Surface with SABAF Burners, Natural Gas Stove for Countertop, Home Improvement Essentials, Easy to Clean, 36 x 4.1 x 20.5\\ncooktop Gas powered with 4 fast burners and 1 ultra-fast center burner Tempered glass surface with removable grid for easy cleaning Lightweight for easy installation. Installation Manual Included Counter cutout Dimensions 19 3/8 x 34 1/2 (see diagram) Insured shipping for your satisfaction and peace of mind Brand Name ABBA EST. 1956, Weight 30 pounds, Dimensions 20.5\\\\ D x 36\\\\ W x 4.1\\\\ H, Installation Type Count\"}, {\"role\": \"assistant\", \"content\": \"Price is $405.00\"}]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e5b56c-8a0b-4d8e-a112-ce87efb4e152",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the items into a list of json objects - a \"jsonl\" string\n",
    "# Each row represents a message in the form:\n",
    "# {\"messages\" : [{\"role\": \"system\", \"content\": \"You estimate prices...\n",
    "\n",
    "\n",
    "def make_jsonl(items):\n",
    "    result = \"\"\n",
    "    for item in items:\n",
    "        messages = messages_for(item)\n",
    "        messages_str = json.dumps(messages)\n",
    "        result += '{\"messages\": ' + messages_str +'}\\n'\n",
    "    return result.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e72de93-a6a6-4b35-855e-15786b97bf5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(make_jsonl(train[:3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "befa532a-9cc7-49a5-ae16-b75be3ab8554",
   "metadata": {},
   "source": [
    "3. Convert the items into jsonl and write them to a file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7734bff0-95c4-4e67-a87e-7e2254e2c67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the items into jsonl and write them to a file\n",
    "\n",
    "def write_jsonl(items, filename):\n",
    "    with open(filename, \"w\") as f:\n",
    "        jsonl = make_jsonl(items)\n",
    "        f.write(jsonl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393d3ad8-999a-4f99-8c04-339d9166d604",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_jsonl(fine_tune_train, \"fine_tune_train.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e23927f-d73e-4668-ac20-abe6f14a56cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_jsonl(fine_tune_validation, \"fine_tune_validation.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59ad8d2-c61a-448e-b7ed-232f1606970f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"fine_tune_train.jsonl\", \"rb\") as f:\n",
    "    train_file = openai.files.create(file=f, purpose=\"fine-tune\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083fefba-fd54-47ce-9ff3-aabbc200846f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b783790-0101-438c-92c6-eb59b0f31004",
   "metadata": {},
   "source": [
    "4. Upload training and validation files to to OpenAI:\n",
    "\n",
    "> FileObject(id='file-ANZ1fXNvT9rpFa5HptJ8fp', bytes=188742, created_at=1745807152, filename='fine_tune_train.jsonl', object='file', purpose='fine-tune', status='processed', expires_at=None, status_details=None)\n",
    "\n",
    "> FileObject(id='file-MGCRJXFiCjosPToMF5ywab', bytes=47085, created_at=1745807167, filename='fine_tune_validation.jsonl', object='file', purpose='fine-tune', status='processed', expires_at=None, status_details=None)\n",
    "\n",
    "Note: rb means open it as a binary file to stream the files to OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97df3360-0760-4422-a556-5f26d23de6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"fine_tune_validation.jsonl\", \"rb\") as f:\n",
    "    validation_file = openai.files.create(file=f, purpose=\"fine-tune\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1abb8f3-9e52-4061-970c-fcf399d8ffa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466052b9-9fb9-48f6-8cf9-c74e6ddc1394",
   "metadata": {},
   "source": [
    "## Step 2: Fine-Tune OpenAI Model\n",
    "\n",
    "To monitor fine-tuning register [Weights & Biases](https://wandb.ai) key on [OpenAI dashboard](https://platform.openai.com/account/organization) under Integrations on the General settings. Then create a wandb project.\n",
    "\n",
    "Note: W&B service accounts (keys) are not available on the free tier.\n",
    "\n",
    "Note: Validation file not necessary in this case but good to get in the habit of sending it\n",
    "\n",
    "- With smaller training datasets usually multiple epochs are run but fixed to 1 below as a good sized trainign set used and can always run more training jobs with new training data. Remove it to let OpenAI decide how many epochs to use (number of traiing runs with data provided).\n",
    "- Hyperparameters are what data scientists call the extra knobs and wheels and settings that control how your training is going to work. Any extra parameter you can set try different posibilities to see if it makes the model better or worse. This process of tryign our different values for better or worse is called _hyperparameter optimization_ or _hyperparameter tuning_, which is just fancy talk to _trial and error_.\n",
    "- Remove integrations line if not monitoring with wandb\n",
    "- Suffix is optinal too and will include this in the name of the model it creates\n",
    "\n",
    "1. Execute fine tuning job:\n",
    "\n",
    "> FineTuningJob(id='ftjob-PANNaovITUNfgU2fPnALCEDL', created_at=1745810086, error=Error(code=None, message=None, param=None), fine_tuned_model=None, finished_at=None, hyperparameters=Hyperparameters(batch_size='auto', learning_rate_multiplier='auto', n_epochs=1), model='gpt-4o-mini-2024-07-18', object='fine_tuning.job', organization_id='org-UqfuFiZ3lRmy0bdF1SFE6SyU', result_files=[], seed=42, status='validating_files', trained_tokens=None, training_file='file-ANZ1fXNvT9rpFa5HptJ8fp', validation_file='file-MGCRJXFiCjosPToMF5ywab', estimated_finish=None, integrations=[FineTuningJobWandbIntegrationObject(type='wandb', wandb=FineTuningJobWandbIntegration(project='gpt-pricer', entity=None, name=None, tags=None, run_id='ftjob-PANNaovITUNfgU2fPnALCEDL'))], metadata=None, method=Method(dpo=None, supervised=MethodSupervised(hyperparameters=MethodSupervisedHyperparameters(batch_size='auto', learning_rate_multiplier='auto', n_epochs=1)), type='supervised'), user_provided_suffix='pricer')\n",
    "\n",
    "> SyncCursorPage[FineTuningJob](data=[FineTuningJob(id='ftjob-PANNaovITUNfgU2fPnALCEDL', created_at=1745810086, error=Error(code=None, message=None, param=None), fine_tuned_model=None, finished_at=None, hyperparameters=Hyperparameters(batch_size='auto', learning_rate_multiplier='auto', n_epochs=1), model='gpt-4o-mini-2024-07-18', object='fine_tuning.job', organization_id='org-UqfuFiZ3lRmy0bdF1SFE6SyU', result_files=[], seed=42, status='validating_files', trained_tokens=None, training_file='file-ANZ1fXNvT9rpFa5HptJ8fp', validation_file='file-MGCRJXFiCjosPToMF5ywab', estimated_finish=None, integrations=[FineTuningJobWandbIntegrationObject(type='wandb', wandb=FineTuningJobWandbIntegration(project='gpt-pricer', entity=None, name=None, tags=None, run_id='ftjob-PANNaovITUNfgU2fPnALCEDL'))], metadata=None, method=Method(dpo=None, supervised=MethodSupervised(hyperparameters=MethodSupervisedHyperparameters(batch_size='auto', learning_rate_multiplier='auto', n_epochs=1)), type='supervised'), user_provided_suffix='pricer')], has_more=False, object='list')\n",
    "\n",
    "> 'ftjob-PANNaovITUNfgU2fPnALCEDL'\n",
    "\n",
    "Start of job events:\n",
    "\n",
    "> [FineTuningJobEvent(id='ftevent-qMjnvvRqSr0yRbcQngQTSJor', created_at=1745810086, level='info', message='Validating training file: file-ANZ1fXNvT9rpFa5HptJ8fp and validation file: file-MGCRJXFiCjosPToMF5ywab', object='fine_tuning.job.event', data={}, type='message'),\n",
    ">\n",
    "> > FineTuningJobEvent(id='ftevent-Qm93jkKFgINt9K1QhswVUU2U', created_at=1745810086, level='info', message='Created fine-tuning job: ftjob-PANNaovITUNfgU2fPnALCEDL', object='fine_tuning.job.event', data={}, type='message')]\n",
    "\n",
    "In progess job events:\n",
    "\n",
    "> [FineTuningJobEvent(id='ftevent-r70gslmVFgjy6rnD8uYnQnio', created_at=1745810442, level='info', message='Step 60/200: training loss=0.73, validation loss=1.46', object='fine_tuning.job.event', data={'step': 60, 'train_loss': 0.7305774688720703, 'valid_loss': 1.4598660469055176, 'total_steps': 200, 'train_mean_token_accuracy': 0.875, 'valid_mean_token_accuracy': 0.75}, type='metrics'),\n",
    "> FineTuningJobEvent(id='ftevent-cLCUQkaOor0OjN2jYOPvVaO1', created_at=1745810440, level='info', message='Step 59/200: training loss=1.40', object='fine_tuning.job.event', data={'step': 59, 'train_loss': 1.401196002960205, 'total_steps': 200, 'train_mean_token_accuracy': 0.75}, type='metrics'),\n",
    "> FineTuningJobEvent(id='ftevent-5J9XiTALuGdFxCpc3Ss60DZ9', created_at=1745810437, level='info', message='Step 58/200: training loss=0.70', object='fine_tuning.job.event', data={'step': 58, 'train_loss': 0.7029500007629395, 'total_steps': 200, 'train_mean_token_accuracy': 0.875}, type='metrics'),\n",
    "> FineTuningJobEvent(id='ftevent-F2F9c4hUPA7E5c4lRHJXpz1W', created_at=1745810437, level='info', message='Step 57/200: training loss=1.14', object='fine_tuning.job.event', data={'step': 57, 'train_loss': 1.1434681415557861, 'total_steps': 200, 'train_mean_token_accuracy': 0.75}, type='metrics'),\n",
    "> FineTuningJobEvent(id='ftevent-bek67WqxTyneLmi0gzasG2NC', created_at=1745810437, level='info', message='Step 56/200: training loss=0.87', object='fine_tuning.job.event', data={'step': 56, 'train_loss': 0.8705952167510986, 'total_steps': 200, 'train_mean_token_accuracy': 0.875}, type='metrics'),\n",
    "> FineTuningJobEvent(id='ftevent-qEghJUp6zY22RAUoGpl3x4Yc', created_at=1745810434, level='info', message='Step 55/200: training loss=0.90', object='fine_tuning.job.event', data={'step': 55, 'train_loss': 0.9008762836456299, 'total_steps': 200, 'train_mean_token_accuracy': 0.875}, type='metrics'),\n",
    "> FineTuningJobEvent(id='ftevent-i8seByn9xcNDgQlMbpOZRaaj', created_at=1745810434, level='info', message='Step 54/200: training loss=1.44', object='fine_tuning.job.event', data={'step': 54, 'train_loss': 1.4444801807403564, 'total_steps': 200, 'train_mean_token_accuracy': 0.75}, type='metrics'),\n",
    "> FineTuningJobEvent(id='ftevent-G9onFQFBU6gukn1AAsxfSAdA', created_at=1745810434, level='info', message='Step 53/200: training loss=1.28', object='fine_tuning.job.event', data={'step': 53, 'train_loss': 1.2828950881958008, 'total_steps': 200, 'train_mean_token_accuracy': 0.75}, type='metrics'),\n",
    "> FineTuningJobEvent(id='ftevent-998fysaBBM2kG2rIDdVx7bes', created_at=1745810431, level='info', message='Step 52/200: training loss=0.63', object='fine_tuning.job.event', data={'step': 52, 'train_loss': 0.6335906982421875, 'total_steps': 200, 'train_mean_token_accuracy': 0.875}, type='metrics'),\n",
    "> FineTuningJobEvent(id='ftevent-medyvOgQsB7pNNMc1njFzB56', created_at=1745810431, level='info', message='Step 51/200: training loss=1.47', object='fine_tuning.job.event', data={'step': 51, 'train_loss': 1.4744696617126465, 'total_steps': 200, 'train_mean_token_accuracy': 0.75}, type='metrics')\n",
    "\n",
    "Completed job events:\n",
    "\n",
    "> [FineTuningJobEvent(id='ftevent-bqb3dmLTCQ2ZCVaVbtATMcPJ', created_at=1745810672, level='info', message='The job has successfully completed', object='fine_tuning.job.event', data={}, type='message'),\n",
    ">\n",
    "> > FineTuningJobEvent(id='ftevent-ARwZfcP8WhglSCqT6vK5soEu', created_at=1745810668, level='info', message='New fine-tuned model created', object='fine_tuning.job.event', data={}, type='message'),\n",
    "> > FineTuningJobEvent(id='ftevent-RGffErhX5N3vxv4vkx9iGo6m', created_at=1745810630, level='info', message='Step 200/200: training loss=1.14, validation loss=1.13, full validation loss=1.12', object='fine_tuning.job.event', data={'step': 200, 'train_loss': 1.1376311779022217, 'valid_loss': 1.1295273303985596, 'total_steps': 200, 'full_valid_loss': 1.1193085956573485, 'train_mean_token_accuracy': 0.75, 'valid_mean_token_accuracy': 0.75, 'full_valid_mean_token_accuracy': 0.7925}, type='metrics'),\n",
    "> > FineTuningJobEvent(id='ftevent-SEA5eFcvrlKYbGA2O8XXNUiJ', created_at=1745810621, level='info', message='Step 199/200: training loss=1.42', object='fine_tuning.job.event', data={'step': 199, 'train_loss': 1.4206628799438477, 'total_steps': 200, 'train_mean_token_accuracy': 0.75}, type='metrics'),\n",
    "> > FineTuningJobEvent(id='ftevent-6gCQzSFkCtI0pwZ5zRz0TTCY', created_at=1745810619, level='info', message='Step 198/200: training loss=0.52', object='fine_tuning.job.event', data={'step': 198, 'train_loss': 0.5175371170043945, 'total_steps': 200, 'train_mean_token_accuracy': 0.875}, type='metrics'),\n",
    "> > FineTuningJobEvent(id='ftevent-53YUlCNBAVkBWHjo6RGdmmIu', created_at=1745810619, level='info', message='Step 197/200: training loss=1.24', object='fine_tuning.job.event', data={'step': 197, 'train_loss': 1.2442662715911865, 'total_steps': 200, 'train_mean_token_accuracy': 0.75}, type='metrics'),\n",
    "> > FineTuningJobEvent(id='ftevent-tEVFXrITeqCn1wiCiFCtdhlY', created_at=1745810619, level='info', message='Step 196/200: training loss=0.87', object='fine_tuning.job.event', data={'step': 196, 'train_loss': 0.8681986331939697, 'total_steps': 200, 'train_mean_token_accuracy': 0.875}, type='metrics'),\n",
    "> > FineTuningJobEvent(id='ftevent-pCTwhQtmrviENqKHuOwPFHmO', created_at=1745810616, level='info', message='Step 195/200: training loss=1.25', object='fine_tuning.job.event', data={'step': 195, 'train_loss': 1.2511980533599854, 'total_steps': 200, 'train_mean_token_accuracy': 0.75}, type='metrics'),\n",
    "> > FineTuningJobEvent(id='ftevent-VKcs3M6Zyw1UXyooc9ZV3BQe', created_at=1745810616, level='info', message='Step 194/200: training loss=0.99', object='fine_tuning.job.event', data={'step': 194, 'train_loss': 0.9867780208587646, 'total_steps': 200, 'train_mean_token_accuracy': 0.875}, type='metrics'),\n",
    "> > FineTuningJobEvent(id='ftevent-Xe26DwmMS5OJizJDt9BrdebI', created_at=1745810616, level='info', message='Step 193/200: training loss=1.23', object='fine_tuning.job.event', data={'step': 193, 'train_loss': 1.2297937870025635, 'total_steps': 200, 'train_mean_token_accuracy': 0.75}, type='metrics')]\n",
    "\n",
    "<img src=\"./../images/Product-Pricer-Fine-Tuning-WandB-Job-Run.jpg\" alt=\"OpenAI fine-tuning training job run monitored in Weigts and Bias\" />\n",
    "\n",
    "Better view of progress from wandb. What to look for:\n",
    "\n",
    "- First few batch steps should show dramatic drop in train loss where model is leanring the structure and obvious stuff about the construct, like it is in dollars, prices and usually close to whole dollars, etc.\n",
    "- What you are really looking for is continual progress from that point\n",
    "- Good to see some variation, some batch steps with greater and lower loss, because your tryign to optimize and explore different possibilities\n",
    "- Look for trend over time a gradual decrease in loss\n",
    "- What could be a concern is if loss doesn't seem to be goign down\n",
    "- Will pause at end of training to perform validation then send you an email whn the run completes\n",
    "\n",
    "<img src=\"./../images/Product-Pricer-Fine-Tuning-WandB-Train-Loss-Graph.jpg\" alt=\"OpenAI fine-tuning training train loss graph in Weights and Bias\" />\n",
    "\n",
    "Results:\n",
    "\n",
    "- Good loss for most of the run\n",
    "- Tappers off after 140 and gets a little worseafter 150(?!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7add1a7-a746-4d6e-a5f8-e25629b8b527",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb_integration = {\"type\": \"wandb\", \"wandb\": {\"project\": \"gpt-pricer\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49801e69-9277-4deb-9f33-99efb6b45ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45421b86-5531-4e42-ab19-d6abbb8f4c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.fine_tuning.jobs.create(\n",
    "    training_file=train_file.id,\n",
    "    validation_file=validation_file.id,\n",
    "    model=\"gpt-4o-mini-2024-07-18\",\n",
    "    seed=42,\n",
    "    hyperparameters={\"n_epochs\": 1},\n",
    "#    integrations = [wandb_integration],\n",
    "    suffix=\"pricer\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb9de2e-542c-4e83-81c7-b6745133e48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.fine_tuning.jobs.list(limit=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d24873-8ff5-413f-b0d4-8f77c28f18e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_id = openai.fine_tuning.jobs.list(limit=1).data[0].id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32aef35-4b38-436c-ad00-d082f758efa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e01247-c133-48e1-93d3-c79c399e6178",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.fine_tuning.jobs.retrieve(job_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5150e1-b8de-485f-8eba-cf1e5b00c117",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.fine_tuning.jobs.list_events(fine_tuning_job_id=job_id, limit=10).data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066fef03-8338-4526-9df3-89b649ad4f0a",
   "metadata": {},
   "source": [
    "## Step 3: Test our fine tuned model\n",
    "\n",
    "Note: this can take a few hours to complete.\n",
    "\n",
    "1. Get model name from job ID (or email from job complete notification):\n",
    "\n",
    "> 'ft:gpt-4o-mini-2024-07-18:personal:pricer:BR9OWYkF'<br>\n",
    "> 'ft:gpt-4o-mini-2024-07-18:personal:pricer:BmQm5j2P'<br>\n",
    "> 'ft:gpt-4o-mini-2024-07-18:personal:pricer:BmQpBfzU'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4488cb-3c17-4eda-abd1-53c1c68a491b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tuned_model_name = openai.fine_tuning.jobs.retrieve(job_id).fine_tuned_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9370937-5a6f-4724-8265-b208663b4450",
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tuned_model_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d666811-2c2b-468f-b2f6-bfe20e502c40",
   "metadata": {},
   "source": [
    "2. Update function to create prompt without price:\n",
    "\n",
    "> [{'role': 'system',<br>\n",
    ">   'content': 'You estimate prices of items. Reply only with the price, no explanation'},<br>\n",
    "> {'role': 'user',<br>\n",
    ">  'content': \"How much does this cost?\\n\\nOEM AC Compressor w/A/C Repair Kit For Ford F150 F-150 V8 & Lincoln Mark LT 2007 2008 - BuyAutoParts NEW\\nAs one of the world's largest automotive parts suppliers, our parts are trusted every day by mechanics and vehicle owners worldwide. This A/C Compressor and Components Kit is manufactured and tested to the strictest OE standards for unparalleled performance. Built for trouble-free ownership and 100% visually inspected and quality tested, this A/C Compressor and Components Kit is backed by our 100% satisfaction guarantee. Guaranteed Exact Fit for easy installation 100% BRAND NEW, premium ISO/TS 16949 quality - tested to meet or exceed OEM specifications Engineered for superior durability, backed by industry-leading unlimited-mileage warranty Included in this K\"},<br>\n",
    "> {'role': 'assistant', 'content': 'Price is $'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ea68e8-ab1b-4f0d-aba4-a59574d8f85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The prompt\n",
    "\n",
    "def messages_for(item):\n",
    "    system_message = \"You estimate prices of items. Reply only with the price, no explanation\"\n",
    "    user_prompt = item.test_prompt().replace(\" to the nearest dollar\",\"\").replace(\"\\n\\nPrice is $\",\"\")\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "        {\"role\": \"assistant\", \"content\": \"Price is $\"}\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff92d61-0d27-4b0d-8b32-c9891016509b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try this out\n",
    "\n",
    "messages_for(test[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37a26b8-1c4a-41d5-b85e-e7af93ebacec",
   "metadata": {},
   "source": [
    "3. Create a utility function to extract the price from a string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1af1888-f94a-4106-b0d8-8a70939eec4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A utility function to extract the price from a string\n",
    "\n",
    "def get_price(s):\n",
    "    s = s.replace('$','').replace(',','')\n",
    "    match = re.search(r\"[-+]?\\d*\\.\\d+|\\d+\", s)\n",
    "    return float(match.group()) if match else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f138c5b7-bcc1-4085-aced-68dad1bf36b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_price(\"The price is roughly $99.99 because blah blah\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1fe63e-0ca6-417a-83f6-3e067bc7ce36",
   "metadata": {},
   "source": [
    "4. Create the function for our fine tuned gpt-4o-mini model:\n",
    "\n",
    "> 374.41<br>\n",
    "> print(gpt_fine_tuned(test[0]))<br>\n",
    "> 174.77"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501a2a7a-69c8-451b-bbc0-398bcb9e1612",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The function for gpt-4o-mini\n",
    "\n",
    "def gpt_fine_tuned(item):\n",
    "    response = openai.chat.completions.create(\n",
    "        model=fine_tuned_model_name, \n",
    "        messages=messages_for(item),\n",
    "        seed=42,\n",
    "        max_tokens=7\n",
    "    )\n",
    "    reply = response.choices[0].message.content\n",
    "    return get_price(reply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843d88b4-364a-431b-b48b-8a7c1f68b786",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test[0].price)\n",
    "print(gpt_fine_tuned(test[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd7ada0-15b7-42ec-bbbb-1250e0eb9af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test[0].test_prompt())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce8b427-2f9f-4768-8672-53a3c6ea59c0",
   "metadata": {},
   "source": [
    "5. Test the model against all the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36bdd2c9-1859-4f99-a09f-3ec83b845b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "Tester.test(gpt_fine_tuned, test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
